{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab10.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sBOvJdJfkXIL",
        "kFoEeTYHDq2s",
        "7z6g7a_Y84n0",
        "TBigIUFTukeJ",
        "zcz0JGXjxFGe",
        "2vJVbYcAJAf2",
        "gMOzGDND9FD1",
        "eOtl8z8G9wbr",
        "2Krh0eYy18R9",
        "aEDv_-H7BvM0",
        "YH5mQBaa-_0b",
        "XMKRI77_-8nc",
        "qBT5jgifC7Im",
        "_K7F19SPQo6U",
        "9YLXvK51RnuL",
        "AHmjSVf_FNHv",
        "gPXJkNubFyY6",
        "iD45m3IwF9hh",
        "usQE-rSPZq_X",
        "IoA0tZZCa_1k",
        "8pa5vFJ5EUjv"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwpv0GZ_CrT"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab10.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnoEhAVvBcMj"
      },
      "source": [
        "# Lab 10: Transfer Learning/Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBOvJdJfkXIL"
      },
      "source": [
        "## Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiuvTUWOjtBC"
      },
      "source": [
        "### Objective\n",
        "\n",
        "- Gain experience fine-tuning pre-trained models to domain-specific applications.\n",
        "\n",
        "### Deliverable\n",
        "\n",
        "For this lab you will submit an ipython notebook via learning suite. The bulk of the work is in modifying fine-tuning a pre-trained ResNet. Fine-tuning the GPT-2 language model is pretty easy. The provided code works as is; you will just have to swap in your own text dataset.\n",
        "\n",
        "### Grading\n",
        "\n",
        "- 35% Create a dataset class for your own dataset\n",
        "- 35% Create a network class that wraps a pretrained ResNet\n",
        "- 20% Implement unfreezing in the network class\n",
        "- 10% Fine-tune GPT-2 on your own dataset\n",
        "\n",
        "### Tips\n",
        "- Your life will be better if you download a dataset that already has the data in the expected format for ImageFolder (make sure to read the documentation!). The datasets recommended below are in the correct format.\n",
        "- Get the CNN working on the provided dataset (bird species classification) before swapping in your own.\n",
        "- For reference on freezing/unfreezing network weights, see [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c)\n",
        "- For training GPT-2, first try the medium-size (355M parameter) model. If your Colab instance doesn't have enough GPU space, you may need to switch to the small-size (124M parameter) model, but the results will be less impressive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzRORuLBNLR"
      },
      "source": [
        "from torchvision.models import resnet152\n",
        "from torchvision.models import resnet18\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pdb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4R3D8Mr8b54"
      },
      "source": [
        "## 1 Fine-tune a ResNet for image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFoEeTYHDq2s"
      },
      "source": [
        "### 1.1 Find a dataset to fine-tune on, and make a Dataset class (1 hr.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z6g7a_Y84n0"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8NtFZRd5hcm"
      },
      "source": [
        "#### DONE:\n",
        "- Inherit from torch.utils.data.Dataset\n",
        "- Use a [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder)\n",
        "- Don't spend too long finding another dataset. Some suggestions that you are free to use:\n",
        " - https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
        " - https://www.kaggle.com/puneet6060/intel-image-classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBigIUFTukeJ"
      },
      "source": [
        "#### Help for downloading kaggle datasets\n",
        "Downloading Kaggle datasets requires authentication, so you can't just download from a url. Here are some step-by-step instructions of how to get Kaggle datasets in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X29UC6CvwfQ"
      },
      "source": [
        "1. Create an API key in Kaggle\n",
        "    - Click on profile photo\n",
        "    - Go to 'My Account'\n",
        "    - Scroll down to the API access section and click \"Create New API Token\"\n",
        "    - `kaggle.json` is now downloaded to your computer\n",
        "\n",
        "2. Upload the API key and install the Kaggle API client by running the next cell (run it again if it throws an error the first time). Also, `files.upload()` may not work in Firefox. One solution is to expand the Files banner (indicated by the '>' tab on the left side of the page) and use that to upload the key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhjc0pM7jOoZ"
      },
      "source": [
        "# # Run this cell and select the kaggle.json file downloaded\n",
        "# # from the Kaggle account settings page.\n",
        "# from google.colab import files\n",
        "# files.upload()\n",
        "# # Next, install the Kaggle API client.\n",
        "# !pip install -q kaggle\n",
        "# # Let's make sure the kaggle.json file is present.\n",
        "# !ls -lha kaggle.json\n",
        "# # The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# # so move it there.\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !mv kaggle.json ~/.kaggle/\n",
        "# # This permissions change avoids a warning on Kaggle tool startup.\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGlIa4SIwEXB"
      },
      "source": [
        "3. Copy the desired dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HtB-XdIr1EE"
      },
      "source": [
        "# Example download command for dataset found here: https://www.kaggle.com/akash2907/bird-species-classification\n",
        "# !kaggle datasets download -d akash2907/bird-species-classification"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcz0JGXjxFGe"
      },
      "source": [
        "#### Make the Dataset class\n",
        "See the implementation below for reference, and implement a dataset class for the dataset you choose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthPlsGeK4CX"
      },
      "source": [
        "# class BirdDataset(Dataset):\n",
        "#     def __init__(self, zip_file='bird-species-classification.zip', size=256, train=True, upload=False):\n",
        "#         super(BirdDataset, self).__init__()\n",
        "        \n",
        "#         self.train = train\n",
        "#         extract_dir = os.path.splitext(zip_file)[0]\n",
        "#         if not os.path.exists(extract_dir):\n",
        "#             os.makedirs(extract_dir)\n",
        "#             self.extract_zip(zip_file, extract_dir)\n",
        "#             # Resize the images - originally they are high resolution. We could do this\n",
        "#             # in the DataLoader, but it will read the full-resolution files from disk\n",
        "#             # every time before resizing them, making training slow\n",
        "#             self.resize(extract_dir, size=size)\n",
        "\n",
        "#         postfix = 'train' if train else 'test'\n",
        "            \n",
        "#         if train:\n",
        "#             # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "#             self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'train_data', 'train_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "#         else:\n",
        "#             self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'test_data', 'test_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "#     def extract_zip(self, zip_file, extract_dir):\n",
        "#         print(\"Extracting\", zip_file)\n",
        "#         with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "#             zip_ref.extractall(extract_dir)\n",
        "\n",
        "#     def resize(self, path, size=256):\n",
        "#         \"\"\"Resizes all images in place\"\"\"\n",
        "#         print(\"Resizing images\")\n",
        "#         dirs = os.walk(path)\n",
        "#         for root, dirs, files in os.walk(path):\n",
        "#             for item in files:\n",
        "#                 name = os.path.join(root, item)\n",
        "#                 if os.path.isfile(name):\n",
        "#                     im = Image.open(name)\n",
        "#                     im = ImageOps.fit(im, (size, size))\n",
        "#                     im.save(name[:-3] + 'bmp', 'BMP')\n",
        "#                     os.remove(name)\n",
        "\n",
        "#     def __getitem__(self, i):\n",
        "#         return self.dataset_folder[i]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.dataset_folder)\n",
        "\n",
        "# bird_data = BirdDataset()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jHFdToeDtIF"
      },
      "source": [
        "#########################\n",
        "# Implement your own Dataset\n",
        "# !unzip -n archive.zip -d ./content/\n",
        "#########################"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJVbYcAJAf2"
      },
      "source": [
        "### 1.2 Wrap a pretrained ResNet in an `nn.Module` (30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOzGDND9FD1"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvmDHbl9IyG"
      },
      "source": [
        "#### DONE:\n",
        "- Make a model class that inherits from `nn.Module`\n",
        "- Wrap a pretrained ResNet and swap out the last layer of that network with a layer that maps to the number of classes in your new dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOtl8z8G9wbr"
      },
      "source": [
        "#### Make your model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY-XU4Mwas0j"
      },
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, num_classes, start_frozen=False):\n",
        "        super(CustomResNet, self).__init__()\n",
        "\n",
        "        # Part 1.2\n",
        "        # Load the model - make sure it is pre-trained\n",
        "        self.model = resnet18(pretrained=True)\n",
        "\n",
        "        # Part 1.4\n",
        "        if start_frozen:\n",
        "          for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Part 1.2\n",
        "        # Look at the code of torchvision.models.resnet152 (or print the ResNet object) to find the name of the attribute to override (the last layer of the ResNet)\n",
        "        # Override the output linear layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        \n",
        "    def unfreeze(self, n_layers):\n",
        "        # Part 1.4\n",
        "        # Turn on gradients for the last n_layers\n",
        "        for i, param in enumerate(self.model.parameters()):\n",
        "          if i >= num_parameters(self.model) - n_layers:\n",
        "            param.requires_grad = True\n",
        "          # else:\n",
        "          #   param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Part 1.2\n",
        "        # Pass x through the resnet\n",
        "        return self.model(x)\n",
        "\n",
        "def num_parameters(model):\n",
        "    return sum(1 for p in model.parameters())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Krh0eYy18R9"
      },
      "source": [
        "### 1.3 Read through and run this training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOGrrw2gbIPf"
      },
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "    \"\"\"Gets average accuracy of a vector of predictions\"\"\"\n",
        "    \n",
        "    preds = torch.argmax(y_hat, dim=1)\n",
        "    acc = torch.mean((preds == y_truth).float())\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, objective, val_loader, device):\n",
        "    \"\"\"Gets average accuracy and loss for the validation set\"\"\"\n",
        "\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    # model.eval() so that batchnorm and dropout work in eval mode\n",
        "    model.eval()\n",
        "    # torch.no_grad() to turn off computation graph creation. This allows for temporal\n",
        "    # and spatial complexity improvements, which allows for larger validation batch \n",
        "    # sizes so it’s recommended\n",
        "    with torch.no_grad():\n",
        "        for x, y_truth in val_loader:\n",
        "\n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "            y_hat = model(x)\n",
        "            val_loss = objective(y_hat, y_truth)\n",
        "            val_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return torch.mean(torch.Tensor(val_losses)), torch.mean(torch.Tensor(val_accs))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKESMcKi2E_f"
      },
      "source": [
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "def train(start_frozen=False, model_unfreeze=0):\n",
        "    \"\"\"Fine-tunes a CNN\n",
        "    Args:\n",
        "        start_frozen (bool): whether to start with the network weights frozen.\n",
        "        model_unfreeze (int): the maximum number of network layers to unfreeze\n",
        "    \"\"\"\n",
        "    epochs = 20\n",
        "    # Start with a very low learning rate\n",
        "    lr = .00005\n",
        "    val_every = 3\n",
        "    batch_size = 32\n",
        "    img_size = 224\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Data\n",
        "    my_dataset = datasets.ImageFolder(root='./content/images/Images',\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(img_size),\n",
        "                               transforms.CenterCrop(img_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "                           ]))\n",
        "    num_classes = len(my_dataset.classes)\n",
        "    \n",
        "    will_use = 2000\n",
        "    wont_use = len(my_dataset)-will_use\n",
        "    will, wont = torch.utils.data.random_split(my_dataset, [will_use, wont_use])\n",
        "    train_length = int(0.8 * len(will))\n",
        "    test_length = len(will) - train_length\n",
        "    train_data, test_data = torch.utils.data.random_split(will, [train_length, test_length])\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    # train_dataset = BirdDataset(upload=True, train=True)\n",
        "    # val_dataset = BirdDataset(upload=True, train=False)\n",
        "    # train_loader = DataLoader(train_dataset,\n",
        "    #                           shuffle=True,\n",
        "    #                           num_workers=8,\n",
        "    #                           batch_size=batch_size)\n",
        "    # val_loader = DataLoader(val_dataset,\n",
        "    #                           shuffle=True,\n",
        "    #                           num_workers=8,\n",
        "    #                           batch_size=batch_size)\n",
        "    \n",
        "    # Model\n",
        "    model = CustomResNet(num_classes, start_frozen=start_frozen).to(device)\n",
        "    \n",
        "    # Objective\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(total=len(train_loader) * epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    cnt = 0\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Implement model unfreezing\n",
        "        if epoch < model_unfreeze:\n",
        "            # Part 1.4\n",
        "            model.unfreeze(epoch)\n",
        "        \n",
        "        for x, y_truth in train_loader:\n",
        "        \n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_hat = model(x)\n",
        "            train_loss = objective(y_hat, y_truth)\n",
        "            train_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_accs.append(train_acc)\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            if cnt % val_every == 0:\n",
        "                val_loss, val_acc = evaluate(model, objective, val_loader, device)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "            pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}.'.format(train_loss.item(), train_acc))\n",
        "            pbar.update(1)\n",
        "            cnt += 1\n",
        "\n",
        "    pbar.close()\n",
        "    plt.subplot(121)\n",
        "    plt.plot(np.arange(len(train_accs)), train_accs, label='Train Accuracy')\n",
        "    plt.plot(np.arange(len(train_accs), step=val_every), val_accs, label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')\n",
        "    plt.plot(np.arange(len(train_losses), step=val_every), val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.show()  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei1vqfkPzY_V"
      },
      "source": [
        "# model = CustomResNet(120, start_frozen=False)\n",
        "# print(num_parameters(model))\n",
        "# count = 0\n",
        "# for param in model.parameters():\n",
        "#   count += 1\n",
        "# print(count)\n",
        "# my_dataset = datasets.ImageFolder(root='./content/images/Images',\n",
        "#                            transform=transforms.Compose([\n",
        "#                                transforms.Resize(224),\n",
        "#                                transforms.CenterCrop(224),\n",
        "#                                transforms.ToTensor(),\n",
        "#                                transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "#                            ]))\n",
        "# print(len(my_dataset))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnxeLotchiH"
      },
      "source": [
        "# train(start_frozen=False, model_unfreeze=0)  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEDv_-H7BvM0"
      },
      "source": [
        "### 1.4 Implement Unfreezing (1 hr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5mQBaa-_0b"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_YmE1pe-6LF"
      },
      "source": [
        "Unfreezing is a technique that can be helpful when fine tuning a CNN for a more difficult task with a large amount of data.\n",
        "\n",
        "The idea is that if we allow the network to tweak the earliest layers immediately, before the last FCL has been trained at all, the earliest layers will forget all of the useful features that they learned in order  to provide features that are helpful for the (untrained) FCL.\n",
        "\n",
        "So, rather than training all of the model weights at once, we learn the last fully connected layer, then train that layer together with the second-to-last layer, gradually adding layers until we reach the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMKRI77_-8nc"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaUc8BTYC1bz"
      },
      "source": [
        "#### DONE:\n",
        "- Modify your model class by setting the `requires_grad` attribute of the ResNet to `False`. (but keep `requires_grad = True` for the last layer).\n",
        "- Add a member function to you model class that allows the user to unfreeze weights in the training loop. See [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c) for reference.\n",
        "- Modify your training loop to add logic that calls the `unfreeze` function of the model class (unfreeze one layer every epoch).\n",
        "- Call your train function to fine-tune the ResNet on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBT5jgifC7Im"
      },
      "source": [
        "#### Call your train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg9ySEO_BNDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8e7e01f2-b29c-4c16-c32a-f2a755f96462"
      },
      "source": [
        "############################\n",
        "# train with unfreezing here (should be a single call to your train function)\n",
        "train(start_frozen=True, model_unfreeze=20)\n",
        "############################"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:0.8114, train accuracy:1.0000.: 100%|██████████| 1000/1000 [15:25<00:00,  1.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZItH2lX7k4Yt"
      },
      "source": [
        "You may not see any improvement for your classification task, but unfreezing can help convergence for more difficult image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXHAUf3EEiE"
      },
      "source": [
        "## 2 Fine-tune a language model - (15 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu9usOxtjFHL"
      },
      "source": [
        "In this section you will use the gpt-2-simple package [here](https://github.com/minimaxir/gpt-2-simple) to fine-tune the GPT-2 language model on a domain of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K7F19SPQo6U"
      },
      "source": [
        "### 2.1 Generate text from an the pretrained GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YLXvK51RnuL"
      },
      "source": [
        "#### Run this code to generate text from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDNOb_H5IRvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7cd79c6-023c-4896-fa90-aa286ba44143"
      },
      "source": [
        "!pip install gpt-2-simple\n",
        "\n",
        "# the transformers package is built on top of Tensorflow, and the default TF version \n",
        "# for Colab will soon switch to 2.x. We remedy this with the following magic method\n",
        "%tensorflow_version 1.x \n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.7/dist-packages (0.7.2)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aRJ-c9uRMOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5ebeca-acf9-495e-815a-a518d7ba7f77"
      },
      "source": [
        "# This line is necessary to be able to run a new tf session\n",
        "tf.reset_default_graph()\n",
        "# The medium-sized model. IF you run out of memory, try \"124M\" instead\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "gpt2.generate(sess, model_name=model_name)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "The harmful effects of open health care access are very real; it's not clear how they're being mitigated. We're concerned about the impact of the Affordable Care Act, particularly the practice of allowing corporations to refuse to serve Medicare or Medicaid patients and to withhold public funds from certain programs that benefit people with chronic conditions, such as HIV/AIDS, diabetes and cancer.\n",
            "\n",
            "We must have a principled, bipartisan approach to this problem. We know that the public health care system needs to be run by the American people, not just by corporations. We know that the Affordable Care Act is a tax on Americans and the private sector can do its part. We know that the public health care system needs to be accountable to working Americans, not just the wealthy. And we know that the ACA needs to be addressed by Democrats, not just Republicans.\n",
            "\n",
            "The opinions expressed by columnists are their own and do not represent the views of Townhall.com.<|endoftext|>AUSTIN, Texas – A Texas man who told Texas police he was a child molester was arrested and charged with felony sexual abuse after he allegedly told a police officer he was a child molester, according to Austin police.\n",
            "\n",
            "Jesse Jeffery Jr., 27, of Austin, was arrested Saturday and charged with felony sexual abuse of a child under the age of 16.\n",
            "\n",
            "Jeffery, who has been charged with two counts of first-degree sexual abuse of a child and one count of first-degree sexual abuse of a child, was released from the Austin County Jail Wednesday. He is scheduled to appear in the Austin County Superior Court on February 17.\n",
            "\n",
            "Jeffery, who also faces charges of forcible confinement and failure to appear, told police he was molested by his former girlfriend, who has been identified by police as Edna, at a party she had at the same time, according to court records. She told police she had just left the party and had to come back because Jeffery had been away.\n",
            "\n",
            "Jeffery said he was able to get a message back from Edna that she had been at the party and had told him she was not there for him.\n",
            "\n",
            "During the time that Edna had been at the party, Jeffery told investigators she had not been at the party, and that she had called her ex-boyfriend.\n",
            "\n",
            "\"It was probably a couple of minutes ago,\" Jeffery told police. \"I was not there at her party where I talked to her about it. I could not remember what she said. I had no idea what she said.\"\n",
            "\n",
            "Jeffery said he told Edna he was going to call her for volunteer work and to talk about his problem.\n",
            "\n",
            "\"That's when I heard her say, 'Jeez, I'm not going to tell you about that.' I said, 'What I'm going to tell you is that I think you're a good boy,' \" Jeffery said. \"I said, 'I don't care if you do or not. I will tell you what I think you need to hear.'\"\n",
            "\n",
            "Edna, 28, of Dallas, told police she had called Jeffery to say that she had been at the party and made no request. She later told police that Jeffery said he had tried to leave but Edna said she did not feel like leaving.\n",
            "\n",
            "Edna told police that she had been at the party at a party she had been attending for about three hours and told her friends to get out of the party.\n",
            "\n",
            "\"She said, 'I'm just going to tell you what I think you need to hear,'\" Edna told police. \"She said, 'Jeez, I really don't care if you do or not. I will tell you what I think you need to hear.'\"\n",
            "\n",
            "Jeffery told police that Edna told him that she was not at the party at all, and that she did not feel like leaving. She said she had been at the party to celebrate her birthday and that she called him by phone.\n",
            "\n",
            "\"She said, 'I'm going to call you,' and I said, 'I don't care if you do or not. I will tell you what I think you need to hear,'\" Jeffery said. \"She said, 'Jeez, I really don't care if you do or not. I will tell you what I think you need to hear.'\"\n",
            "\n",
            "Edna said that she had been at the party prior to that call, and that she had been asked to leave the party at that time. She said she had been told by an acquaintance that the party was only for the children at the party.\n",
            "\n",
            "\"I told them, 'I was just kidding. I was just kidding,'\" Edna said. \"She said, 'I don't care if you do or not. I will tell you what I think you need to hear.'\"\n",
            "\n",
            "Edna told police she did not think that her ex-boyfriend told her to leave early,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmjSVf_FNHv"
      },
      "source": [
        "### 2.2 Download a text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPXJkNubFyY6"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkuRjbcFzwb"
      },
      "source": [
        "#### DONE:\n",
        "- Use the provided functions to download your own text dataset\n",
        "- [Project Gutenberg](https://www.gutenberg.org/) is a nice starting point for raw text corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD45m3IwF9hh"
      },
      "source": [
        "#### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESltl2QM5nxw"
      },
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "# from torchvision import datasets\n",
        "\n",
        "# def extract_zip(zip_path, remove_finished=True):\n",
        "#     print('Extracting {}'.format(zip_path))\n",
        "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#         zip_ref.extractall(zip_path.replace('.zip', ''))\n",
        "#     if remove_finished:\n",
        "#         os.remove(zip_path)\n",
        "\n",
        "# def download_dataset(url, root='../data'):\n",
        "#     if not os.path.exists(os.path.join(root, 'text')):\n",
        "#         os.makedirs(os.path.join(root))\n",
        "#         datasets.utils.download_url(url, root, 'text.zip', None)\n",
        "#         extract_zip(os.path.join(root, 'text.zip'))\n",
        "#     return os.path.join(root, 'text')\n",
        "\n",
        "# ##########################################\n",
        "# # Set the url for your dataset here,\n",
        "# # move the dataset to the desired location\n",
        "# ##########################################\n",
        "# # https://www.gutenberg.org/cache/epub/17192/pg17192.txt\n",
        "# url = 'https://www.gutenberg.org/files/30/30.zip'\n",
        "# download_dataset(url)\n",
        "# !mv /data/text/30.txt /data/text/bible.txt\n",
        "# !ls ../data/text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQE-rSPZq_X"
      },
      "source": [
        "### 2.3 Fine-tune GPT-2 on your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoA0tZZCa_1k"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoU6ML1mbgjP"
      },
      "source": [
        "#### DONE:\n",
        "- Swap out the dataset parameter with the path to your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pa5vFJ5EUjv"
      },
      "source": [
        "#### Train on your dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuQ5snl4LuS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef975fc-60fc-45e8-d7ab-f354093a7de6"
      },
      "source": [
        "# This line is necessary to be able to run a new tf session if one has already been run\n",
        "tf.reset_default_graph()\n",
        "# Start a session\n",
        "sess = gpt2.start_tf_sess()\n",
        "# Fine tune `model_name` on `data`\n",
        "###################################\n",
        "# Swap out the `dataset` parameter with the path to your text dataset\n",
        "###################################\n",
        "gpt2.finetune(sess,\n",
        "              dataset='./raven.txt',\n",
        "              model_name=model_name,\n",
        "              restore_from='latest',\n",
        "              steps=500)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint checkpoint/run1/model-172\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 356.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "dataset has 13639 tokens\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1 | 8.86] loss=0.06 avg=0.06\n",
            "[2 | 11.30] loss=0.07 avg=0.06\n",
            "[3 | 13.78] loss=0.08 avg=0.07\n",
            "[4 | 16.27] loss=0.06 avg=0.06\n",
            "[5 | 18.79] loss=0.07 avg=0.07\n",
            "[6 | 21.35] loss=0.07 avg=0.07\n",
            "[7 | 23.91] loss=0.07 avg=0.07\n",
            "[8 | 26.47] loss=0.09 avg=0.07\n",
            "[9 | 29.05] loss=0.13 avg=0.08\n",
            "[10 | 31.65] loss=0.08 avg=0.08\n",
            "[11 | 34.22] loss=0.08 avg=0.08\n",
            "[12 | 36.78] loss=0.06 avg=0.08\n",
            "[13 | 39.31] loss=0.09 avg=0.08\n",
            "[14 | 41.81] loss=0.08 avg=0.08\n",
            "[15 | 44.29] loss=0.07 avg=0.08\n",
            "[16 | 46.77] loss=0.06 avg=0.08\n",
            "[17 | 49.24] loss=0.05 avg=0.07\n",
            "[18 | 51.68] loss=0.08 avg=0.07\n",
            "[19 | 54.10] loss=0.06 avg=0.07\n",
            "[20 | 56.51] loss=0.07 avg=0.07\n",
            "[21 | 58.92] loss=0.05 avg=0.07\n",
            "[22 | 61.32] loss=0.07 avg=0.07\n",
            "[23 | 63.72] loss=0.06 avg=0.07\n",
            "[24 | 66.12] loss=0.07 avg=0.07\n",
            "[25 | 68.51] loss=0.04 avg=0.07\n",
            "[26 | 70.91] loss=0.06 avg=0.07\n",
            "[27 | 73.29] loss=0.06 avg=0.07\n",
            "[28 | 75.68] loss=0.06 avg=0.07\n",
            "[29 | 78.08] loss=0.07 avg=0.07\n",
            "[30 | 80.47] loss=0.05 avg=0.07\n",
            "[31 | 82.88] loss=0.07 avg=0.07\n",
            "[32 | 85.30] loss=0.05 avg=0.07\n",
            "[33 | 87.73] loss=0.05 avg=0.07\n",
            "[34 | 90.16] loss=0.04 avg=0.07\n",
            "[35 | 92.61] loss=0.05 avg=0.06\n",
            "[36 | 95.06] loss=0.04 avg=0.06\n",
            "[37 | 97.52] loss=0.05 avg=0.06\n",
            "[38 | 99.98] loss=0.06 avg=0.06\n",
            "[39 | 102.44] loss=0.04 avg=0.06\n",
            "[40 | 104.92] loss=0.05 avg=0.06\n",
            "[41 | 107.39] loss=0.04 avg=0.06\n",
            "[42 | 109.87] loss=0.04 avg=0.06\n",
            "[43 | 112.35] loss=0.06 avg=0.06\n",
            "[44 | 114.84] loss=0.04 avg=0.06\n",
            "[45 | 117.32] loss=0.05 avg=0.06\n",
            "[46 | 119.80] loss=0.05 avg=0.06\n",
            "[47 | 122.27] loss=0.06 avg=0.06\n",
            "[48 | 124.75] loss=0.05 avg=0.06\n",
            "[49 | 127.23] loss=0.03 avg=0.06\n",
            "[50 | 129.69] loss=0.04 avg=0.06\n",
            "[51 | 132.15] loss=0.05 avg=0.06\n",
            "[52 | 134.61] loss=0.05 avg=0.06\n",
            "[53 | 137.07] loss=0.04 avg=0.06\n",
            "[54 | 139.53] loss=0.05 avg=0.06\n",
            "[55 | 141.99] loss=0.04 avg=0.06\n",
            "[56 | 144.45] loss=0.04 avg=0.06\n",
            "[57 | 146.90] loss=0.04 avg=0.06\n",
            "[58 | 149.35] loss=0.04 avg=0.06\n",
            "[59 | 151.79] loss=0.04 avg=0.06\n",
            "[60 | 154.23] loss=0.04 avg=0.05\n",
            "[61 | 156.66] loss=0.04 avg=0.05\n",
            "[62 | 159.10] loss=0.05 avg=0.05\n",
            "[63 | 161.52] loss=0.04 avg=0.05\n",
            "[64 | 163.96] loss=0.03 avg=0.05\n",
            "[65 | 166.39] loss=0.03 avg=0.05\n",
            "[66 | 168.83] loss=0.03 avg=0.05\n",
            "[67 | 171.27] loss=0.04 avg=0.05\n",
            "[68 | 173.71] loss=0.04 avg=0.05\n",
            "[69 | 176.15] loss=0.03 avg=0.05\n",
            "[70 | 178.59] loss=0.04 avg=0.05\n",
            "[71 | 181.03] loss=0.04 avg=0.05\n",
            "[72 | 183.48] loss=0.03 avg=0.05\n",
            "[73 | 185.94] loss=0.05 avg=0.05\n",
            "[74 | 188.38] loss=0.04 avg=0.05\n",
            "[75 | 190.81] loss=0.05 avg=0.05\n",
            "[76 | 193.25] loss=0.03 avg=0.05\n",
            "[77 | 195.69] loss=0.03 avg=0.05\n",
            "[78 | 198.15] loss=0.03 avg=0.05\n",
            "[79 | 200.59] loss=0.04 avg=0.05\n",
            "[80 | 203.04] loss=0.03 avg=0.05\n",
            "[81 | 205.49] loss=0.04 avg=0.05\n",
            "[82 | 207.93] loss=0.05 avg=0.05\n",
            "[83 | 210.38] loss=0.04 avg=0.05\n",
            "[84 | 212.84] loss=0.03 avg=0.05\n",
            "[85 | 215.29] loss=0.04 avg=0.05\n",
            "[86 | 217.74] loss=0.04 avg=0.05\n",
            "[87 | 220.20] loss=0.02 avg=0.05\n",
            "[88 | 222.66] loss=0.06 avg=0.05\n",
            "[89 | 225.10] loss=0.04 avg=0.05\n",
            "[90 | 227.55] loss=0.07 avg=0.05\n",
            "[91 | 230.01] loss=0.05 avg=0.05\n",
            "[92 | 232.47] loss=0.08 avg=0.05\n",
            "[93 | 234.92] loss=0.04 avg=0.05\n",
            "[94 | 237.38] loss=0.04 avg=0.05\n",
            "[95 | 239.84] loss=0.07 avg=0.05\n",
            "[96 | 242.28] loss=0.04 avg=0.05\n",
            "[97 | 244.73] loss=0.05 avg=0.05\n",
            "[98 | 247.18] loss=0.05 avg=0.05\n",
            "[99 | 249.64] loss=0.04 avg=0.05\n",
            "[100 | 252.09] loss=0.04 avg=0.05\n",
            "======== SAMPLE 1 ========\n",
            " maiden maiden, and it was\n",
            "with a sense of special privilege and special responsibility that I was\n",
            "passive when I mentioned this poem to my young wife,\n",
            "who, with the encouragement of a few friends, soon profited by it\n",
            "and still more by its influence. A few lines of the\n",
            "Song, which in America is called, but in the \"Restatement\" of English\n",
            "Tales, have been added to the original. Still, one marvellous\n",
            "character from the romance of his career, _The Raven_, came\n",
            "out of the woodwork. More than one, while otherwise unique, came\n",
            "out of the blue. The best known was Theodore Roosevelt, the \"Don\" of\n",
            "Russia. He was the first to be lured by the America of romance; the\n",
            "rhythmical structure of which I now see it the prototype, the\n",
            "stage-name, after the one by which Poe came before him, \"The\n",
            "Raven\" would be designated. Through the help of his wife, Virginia, he\n",
            "consigned one beautiful work to the last minute, from the\n",
            "melody of a dream. She also made crocs in some of his later poems,\n",
            "consistent of the late Berserkers. These are the stanzas in theappendices to each of\n",
            "these poems, as given to us by the poet himself:\n",
            "\n",
            "    \"Around, by lifting winds forgot,\n",
            "    Resignedly beneath the sky\n",
            "    The poet's melancholy mood hath No Rated Fan\n",
            "    Swung by the hand of fate.\"\n",
            "\n",
            "Here we have a prolonged measure, a similarity of refrain, and the\n",
            "introduction of a bird whose song enhances sorrow. There are other trails\n",
            "which may be followed by the curious; notably, a passage which Mr. Ingram\n",
            "selects from Poe's final review of \"Barnaby Rudge\":\n",
            "\n",
            "     \"The raven, too, * * * might have been made, more than we now see\n",
            "     it, a portion of the conception of the fantastic Barnaby. * * * Its\n",
            "     character might have performed, in regard to that of the idiot,\n",
            "     much the same part as does, in music, the accompaniment in respect\n",
            "     to the air.\"\n",
            "\n",
            "Nevertheless, after pointing out these germs and resemblances, the value\n",
            "of the poem still is found in its originality. The progressive music, the\n",
            "scenic detail and contrasted light and shade,--above all, the spiritual\n",
            "passion of the nocturn, make it the work of an informing genius. As for the\n",
            "gruesome bird, he is unlike all the other ravens of his clan, from the \"twa\n",
            "corbies\" and \"three ravens\" of the balladists to Barnaby's rumpled \"Grip.\"\n",
            "Here is no semblance of the cawing rook that haunts ancestral turrets and\n",
            "treads the field of heraldry; no boding phantom of which Tickell sang that,\n",
            "when,\n",
            "\n",
            "            \"shrieking at her window thrice,\n",
            "       The raven flap'd his wing,\n",
            "    Too well the love-lorn maiden knew\n",
            "       The solemn boding sound.\"\n",
            "\n",
            "Poe's raven is a distinct conception; the incarnation of a mourner's agony\n",
            "and hopelessness; a sable embodied Memory, the abiding chronicler of doom,\n",
            "a type of the Irreparable. Escaped across the Styx, from \"the Night's\n",
            "Plutonian shore,\" he seems the imaged soul of the questioner himself,--of\n",
            "him who can not, will not, quaff the kind nepenthe, because the memory of\n",
            "Lenore is all that is left him, and with the surcease of his sorrow even\n",
            "that would be put aside.\n",
            "\n",
            "_The Raven_ also may be taken as a representative poem of its author, for\n",
            "its exemplification of all his notions of what a poem should be. These are\n",
            "found in his essays on \"The Poetic Principle,\" \"The Rationale of Verse,\"\n",
            " and \"The Philosophy of Composition.\" Poe declared that \"in Music, perhaps,\n",
            "the soul most nearly attains the great end for which, when inspired by the\n",
            "Poetic Sentiment, it struggles--the creation of supernal Beauty.... Verse\n",
            "cannot be better designated than as an inferior or less capable music\"; but\n",
            "again, verse which is really the \"Poetry of Words\" is \"The Rhythmical\n",
            "Creation of Beauty,\"--this and nothing more. The _tone_ of the highest\n",
            "Beauty is one of Sadness. The most melancholy of topics is Death. This must\n",
            "be allied to Beauty. \"The death, then\n",
            "\n",
            "[101 | 264.85] loss=0.06 avg=0.05\n",
            "[102 | 267.31] loss=0.04 avg=0.05\n",
            "[103 | 269.77] loss=0.05 avg=0.05\n",
            "[104 | 272.21] loss=0.04 avg=0.05\n",
            "[105 | 274.67] loss=0.04 avg=0.05\n",
            "[106 | 277.13] loss=0.04 avg=0.05\n",
            "[107 | 279.59] loss=0.04 avg=0.05\n",
            "[108 | 282.05] loss=0.04 avg=0.05\n",
            "[109 | 284.50] loss=0.04 avg=0.05\n",
            "[110 | 286.96] loss=0.04 avg=0.05\n",
            "[111 | 289.41] loss=0.06 avg=0.05\n",
            "[112 | 291.87] loss=0.05 avg=0.05\n",
            "[113 | 294.33] loss=0.04 avg=0.05\n",
            "[114 | 296.78] loss=0.04 avg=0.05\n",
            "[115 | 299.24] loss=0.03 avg=0.05\n",
            "[116 | 301.70] loss=0.04 avg=0.05\n",
            "[117 | 304.15] loss=0.04 avg=0.05\n",
            "[118 | 306.61] loss=0.04 avg=0.05\n",
            "[119 | 309.07] loss=0.03 avg=0.05\n",
            "[120 | 311.52] loss=0.03 avg=0.05\n",
            "[121 | 313.98] loss=0.04 avg=0.05\n",
            "[122 | 316.44] loss=0.02 avg=0.05\n",
            "[123 | 318.90] loss=0.03 avg=0.05\n",
            "[124 | 321.36] loss=0.03 avg=0.05\n",
            "[125 | 323.81] loss=0.03 avg=0.05\n",
            "[126 | 326.27] loss=0.04 avg=0.05\n",
            "[127 | 328.73] loss=0.04 avg=0.05\n",
            "[128 | 331.19] loss=0.03 avg=0.04\n",
            "[129 | 333.65] loss=0.04 avg=0.04\n",
            "[130 | 336.11] loss=0.03 avg=0.04\n",
            "[131 | 338.57] loss=0.06 avg=0.04\n",
            "[132 | 341.03] loss=0.04 avg=0.04\n",
            "[133 | 343.49] loss=0.03 avg=0.04\n",
            "[134 | 345.95] loss=0.04 avg=0.04\n",
            "[135 | 348.41] loss=0.03 avg=0.04\n",
            "[136 | 350.87] loss=0.05 avg=0.04\n",
            "[137 | 353.32] loss=0.04 avg=0.04\n",
            "[138 | 355.78] loss=0.04 avg=0.04\n",
            "[139 | 358.24] loss=0.04 avg=0.04\n",
            "[140 | 360.70] loss=0.03 avg=0.04\n",
            "[141 | 363.16] loss=0.03 avg=0.04\n",
            "[142 | 365.61] loss=0.04 avg=0.04\n",
            "[143 | 368.07] loss=0.03 avg=0.04\n",
            "[144 | 370.53] loss=0.03 avg=0.04\n",
            "[145 | 372.98] loss=0.04 avg=0.04\n",
            "[146 | 375.44] loss=0.05 avg=0.04\n",
            "[147 | 377.89] loss=0.05 avg=0.04\n",
            "[148 | 380.35] loss=0.03 avg=0.04\n",
            "[149 | 382.80] loss=0.04 avg=0.04\n",
            "[150 | 385.26] loss=0.04 avg=0.04\n",
            "[151 | 387.72] loss=0.03 avg=0.04\n",
            "[152 | 390.18] loss=0.03 avg=0.04\n",
            "[153 | 392.63] loss=0.04 avg=0.04\n",
            "[154 | 395.11] loss=0.03 avg=0.04\n",
            "[155 | 397.57] loss=0.04 avg=0.04\n",
            "[156 | 400.03] loss=0.03 avg=0.04\n",
            "[157 | 402.49] loss=0.04 avg=0.04\n",
            "[158 | 404.95] loss=0.03 avg=0.04\n",
            "[159 | 407.41] loss=0.03 avg=0.04\n",
            "[160 | 409.87] loss=0.05 avg=0.04\n",
            "[161 | 412.33] loss=0.02 avg=0.04\n",
            "[162 | 414.79] loss=0.03 avg=0.04\n",
            "[163 | 417.25] loss=0.05 avg=0.04\n",
            "[164 | 419.70] loss=0.05 avg=0.04\n",
            "[165 | 422.16] loss=0.04 avg=0.04\n",
            "[166 | 424.61] loss=0.05 avg=0.04\n",
            "[167 | 427.07] loss=0.05 avg=0.04\n",
            "[168 | 429.53] loss=0.03 avg=0.04\n",
            "[169 | 431.99] loss=0.04 avg=0.04\n",
            "[170 | 434.44] loss=0.06 avg=0.04\n",
            "[171 | 436.90] loss=0.04 avg=0.04\n",
            "[172 | 439.35] loss=0.05 avg=0.04\n",
            "[173 | 441.81] loss=0.05 avg=0.04\n",
            "[174 | 444.27] loss=0.04 avg=0.04\n",
            "[175 | 446.72] loss=0.03 avg=0.04\n",
            "[176 | 449.18] loss=0.04 avg=0.04\n",
            "[177 | 451.64] loss=0.03 avg=0.04\n",
            "[178 | 454.10] loss=0.04 avg=0.04\n",
            "[179 | 456.55] loss=0.04 avg=0.04\n",
            "[180 | 459.01] loss=0.05 avg=0.04\n",
            "[181 | 461.46] loss=0.05 avg=0.04\n",
            "[182 | 463.92] loss=0.03 avg=0.04\n",
            "[183 | 466.36] loss=0.03 avg=0.04\n",
            "[184 | 468.81] loss=0.03 avg=0.04\n",
            "[185 | 471.27] loss=0.05 avg=0.04\n",
            "[186 | 473.72] loss=0.05 avg=0.04\n",
            "[187 | 476.18] loss=0.04 avg=0.04\n",
            "[188 | 478.63] loss=0.02 avg=0.04\n",
            "[189 | 481.07] loss=0.05 avg=0.04\n",
            "[190 | 483.51] loss=0.04 avg=0.04\n",
            "[191 | 485.97] loss=0.04 avg=0.04\n",
            "[192 | 488.41] loss=0.03 avg=0.04\n",
            "[193 | 490.85] loss=0.04 avg=0.04\n",
            "[194 | 493.30] loss=0.03 avg=0.04\n",
            "[195 | 495.75] loss=0.03 avg=0.04\n",
            "[196 | 498.21] loss=0.02 avg=0.04\n",
            "[197 | 500.65] loss=0.03 avg=0.04\n",
            "[198 | 503.09] loss=0.04 avg=0.04\n",
            "[199 | 505.54] loss=0.05 avg=0.04\n",
            "[200 | 507.99] loss=0.03 avg=0.04\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "\"Other friends have flown before--\n",
            "On the morrow _he_ will leave me, as my hopes have flown before.\"\n",
            "                                           _Frank French._\n",
            "\n",
            "\"Then, upon the velvet sinking, I betook myself to linking\n",
            "Fancy unto fancy.\"                       _R. Schelling._\n",
            "\n",
            "\"But whose velvet violet lining with the lamplight gloating o'er\n",
            "                                        _She_ shall press, ah, nevermore!\"\n",
            "                                                            _George Kruell._\n",
            "\n",
            "\"'Wretch,' I cried, 'thy God hath lent thee--by these angels he hath sent thee\n",
            "Respite--respite and nepenthe from thy memories of Lenore!'\"\n",
            "                                             _Victor Bernstrom._\n",
            "\n",
            "\"On this home by Horror haunted.\"                   _R. Staudenbaur._\n",
            "\n",
            "                               \"'Tell me truly, I implore--\n",
            "Is there--_is_ there balm in Gilead?--tell me--tell me, I implore!'\"\n",
            "                                                  _W. Zimmermann._\n",
            "\n",
            "\"'Tell this soul with sorrow laden if, within the distant Aidenn,\n",
            "It shall clasp a sainted maiden whom the angels name Lenore.'\"\n",
            "                                                  _F.S. King._\n",
            "\n",
            "\"'Be that word our sign of parting, bird or fiend!' I shrieked, upstarting.\"\n",
            "                                                  _W. Zimmermann._\n",
            "\n",
            "\"'Get thee back into the tempest and the Night's Plutonian shore!'\"\n",
            "                                                  _Robert Hoskin._\n",
            "\n",
            "\"And my soul from out that shadow that lies floating on the floor\n",
            "                                Shall be lifted--nevermore!\"\n",
            "                                                  _R.G. Tietze._\n",
            "\n",
            "The secret of the Sphinx.                           _R. Staudenbaur._\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "COMMENT ON THE POEM.\n",
            "\n",
            "\n",
            "The secret of a poem, no less than a jest's prosperity, lies in the ear of\n",
            "him that hears it. Yield to its spell, accept the poet's mood: this, after\n",
            "all, is what the sages answer when you ask them of its value. Even though\n",
            "the poet himself, in his other mood, tell you that his art is but sleight\n",
            "of hand, his food enchanter's food, and offer to show you the trick of\n",
            "\n",
            "[201 | 519.70] loss=0.04 avg=0.04\n",
            "[202 | 522.14] loss=0.03 avg=0.04\n",
            "[203 | 524.57] loss=0.04 avg=0.04\n",
            "[204 | 527.02] loss=0.02 avg=0.04\n",
            "[205 | 529.47] loss=0.03 avg=0.04\n",
            "[206 | 531.92] loss=0.04 avg=0.04\n",
            "[207 | 534.35] loss=0.03 avg=0.04\n",
            "[208 | 536.79] loss=0.03 avg=0.04\n",
            "[209 | 539.26] loss=0.03 avg=0.04\n",
            "[210 | 541.69] loss=0.03 avg=0.04\n",
            "[211 | 544.14] loss=0.03 avg=0.04\n",
            "[212 | 546.58] loss=0.02 avg=0.04\n",
            "[213 | 549.02] loss=0.03 avg=0.04\n",
            "[214 | 551.48] loss=0.03 avg=0.04\n",
            "[215 | 553.93] loss=0.03 avg=0.04\n",
            "[216 | 556.39] loss=0.02 avg=0.04\n",
            "[217 | 558.83] loss=0.03 avg=0.04\n",
            "[218 | 561.28] loss=0.02 avg=0.04\n",
            "[219 | 563.74] loss=0.04 avg=0.04\n",
            "[220 | 566.20] loss=0.03 avg=0.04\n",
            "[221 | 568.65] loss=0.03 avg=0.04\n",
            "[222 | 571.11] loss=0.04 avg=0.04\n",
            "[223 | 573.56] loss=0.03 avg=0.04\n",
            "[224 | 576.02] loss=0.03 avg=0.04\n",
            "[225 | 578.48] loss=0.03 avg=0.04\n",
            "[226 | 580.93] loss=0.04 avg=0.04\n",
            "[227 | 583.39] loss=0.04 avg=0.04\n",
            "[228 | 585.85] loss=0.03 avg=0.04\n",
            "[229 | 588.30] loss=0.03 avg=0.04\n",
            "[230 | 590.76] loss=0.04 avg=0.04\n",
            "[231 | 593.22] loss=0.03 avg=0.04\n",
            "[232 | 595.67] loss=0.02 avg=0.04\n",
            "[233 | 598.13] loss=0.02 avg=0.04\n",
            "[234 | 600.59] loss=0.02 avg=0.04\n",
            "[235 | 603.05] loss=0.02 avg=0.04\n",
            "[236 | 605.51] loss=0.03 avg=0.04\n",
            "[237 | 607.97] loss=0.04 avg=0.04\n",
            "[238 | 610.42] loss=0.04 avg=0.04\n",
            "[239 | 612.89] loss=0.04 avg=0.04\n",
            "[240 | 615.35] loss=0.03 avg=0.04\n",
            "[241 | 617.81] loss=0.02 avg=0.04\n",
            "[242 | 620.26] loss=0.03 avg=0.04\n",
            "[243 | 622.72] loss=0.03 avg=0.04\n",
            "[244 | 625.18] loss=0.03 avg=0.04\n",
            "[245 | 627.63] loss=0.03 avg=0.04\n",
            "[246 | 630.09] loss=0.03 avg=0.04\n",
            "[247 | 632.55] loss=0.04 avg=0.04\n",
            "[248 | 635.01] loss=0.04 avg=0.04\n",
            "[249 | 637.46] loss=0.03 avg=0.04\n",
            "[250 | 639.92] loss=0.04 avg=0.04\n",
            "[251 | 642.38] loss=0.03 avg=0.04\n",
            "[252 | 644.84] loss=0.04 avg=0.04\n",
            "[253 | 647.29] loss=0.05 avg=0.04\n",
            "[254 | 649.75] loss=0.03 avg=0.04\n",
            "[255 | 652.20] loss=0.04 avg=0.04\n",
            "[256 | 654.66] loss=0.03 avg=0.04\n",
            "[257 | 657.12] loss=0.02 avg=0.04\n",
            "[258 | 659.57] loss=0.03 avg=0.04\n",
            "[259 | 662.03] loss=0.03 avg=0.04\n",
            "[260 | 664.49] loss=0.03 avg=0.04\n",
            "[261 | 666.94] loss=0.03 avg=0.04\n",
            "[262 | 669.40] loss=0.03 avg=0.04\n",
            "[263 | 671.86] loss=0.03 avg=0.04\n",
            "[264 | 674.32] loss=0.03 avg=0.04\n",
            "[265 | 676.79] loss=0.03 avg=0.04\n",
            "[266 | 679.25] loss=0.03 avg=0.04\n",
            "[267 | 681.69] loss=0.04 avg=0.04\n",
            "[268 | 684.15] loss=0.04 avg=0.04\n",
            "[269 | 686.61] loss=0.04 avg=0.04\n",
            "[270 | 689.06] loss=0.03 avg=0.04\n",
            "[271 | 691.51] loss=0.02 avg=0.04\n",
            "[272 | 693.97] loss=0.03 avg=0.04\n",
            "[273 | 696.43] loss=0.04 avg=0.04\n",
            "[274 | 698.88] loss=0.02 avg=0.04\n",
            "[275 | 701.34] loss=0.03 avg=0.04\n",
            "[276 | 703.79] loss=0.03 avg=0.04\n",
            "[277 | 706.25] loss=0.02 avg=0.04\n",
            "[278 | 708.71] loss=0.03 avg=0.04\n",
            "[279 | 711.16] loss=0.03 avg=0.04\n",
            "[280 | 713.61] loss=0.03 avg=0.04\n",
            "[281 | 716.08] loss=0.03 avg=0.04\n",
            "[282 | 718.53] loss=0.03 avg=0.04\n",
            "[283 | 720.98] loss=0.04 avg=0.04\n",
            "[284 | 723.44] loss=0.03 avg=0.04\n",
            "[285 | 725.89] loss=0.03 avg=0.04\n",
            "[286 | 728.36] loss=0.02 avg=0.03\n",
            "[287 | 730.80] loss=0.03 avg=0.03\n",
            "[288 | 733.25] loss=0.03 avg=0.03\n",
            "[289 | 735.70] loss=0.03 avg=0.03\n",
            "[290 | 738.16] loss=0.03 avg=0.03\n",
            "[291 | 740.62] loss=0.03 avg=0.03\n",
            "[292 | 743.08] loss=0.03 avg=0.03\n",
            "[293 | 745.53] loss=0.03 avg=0.03\n",
            "[294 | 747.98] loss=0.02 avg=0.03\n",
            "[295 | 750.44] loss=0.03 avg=0.03\n",
            "[296 | 752.90] loss=0.03 avg=0.03\n",
            "[297 | 755.36] loss=0.03 avg=0.03\n",
            "[298 | 757.81] loss=0.03 avg=0.03\n",
            "[299 | 760.25] loss=0.03 avg=0.03\n",
            "[300 | 762.70] loss=0.03 avg=0.03\n",
            "======== SAMPLE 1 ========\n",
            "                           Then the bird said, \"Nevermore.\"\n",
            "\n",
            "Startled at the stillness broken by reply so aptly spoken,\n",
            "\"Doubtless,\" said I, \"what it utters is its only stock and store,\n",
            "Caught from some unhappy master whom unmerciful Disaster\n",
            "Followed fast and followed faster till his songs one burden bore--\n",
            "Till the dirges of his Hope that melancholy burden bore\n",
            "                                      Of 'Never--nevermore.'\"\n",
            "\n",
            "But the Raven still beguiling all my sad soul into smiling,\n",
            "Straight I wheeled a cushioned seat in front of bird and bust and door;\n",
            "Then, upon the velvet sinking, I betook myself to linking\n",
            "Fancy unto fancy, thinking what this ominous bird of yore--\n",
            "What this grim, ungainly, ghastly, gaunt and ominous bird of yore\n",
            "                                      Is the plaint of a\n",
            "charm,--the sable plaint of a wife and family engaged in native\n",
            "kindwort;--\n",
            "                                    With such name as \"Nevermore.\"\n",
            "\n",
            "But the Raven still beguiling all my sad soul into smiling,\n",
            "Straight I stood still, immured within my seat, immured above the floor,\n",
            "And, with mien of lord or lady, spoke these words:\n",
            "\n",
            "    \"Once upon a midnight dreary, while I pondered, weak and weary,\n",
            "    Over many a quaint and curious volume of forgotten lore.\"\n",
            "\n",
            "This I uttered, and then with a murmur, said, \"Sir,\"--\n",
            "\"That is what it is, O sleepy brain;--\n",
            "                                      Of 'Never--nevermore.'\"\n",
            "\n",
            "And the Raven still beguiling all my sad soul into smiling,\n",
            "Straight I stood still, immured within my seat, immured above the floor,\n",
            "And with the very feather on my forehead, I said,\n",
            "\"Surely,\" said the Raven, \"surely that is something at my window lattice;\n",
            "    Let me see, then, what thereat is, and this mystery explore;\n",
            "    Then tell me what this mystery mystery mystery                                       'T is the wind and nothing more!\"\n",
            "\n",
            "Open here I flung the shutter, when, with many a flirt and flutter,\n",
            "In there stepped a stately Raven of the saintly days of yore.\n",
            "Not the least obeisance made he; not a minute stopped or stayed he;\n",
            "But, with mien of lord or lady, perched above my chamber door--\n",
            "Perched upon a bust of Pallas just above my chamber door--\n",
            "                                         Perched, and sat, and nothing more.\n",
            "\n",
            "Then this ebony bird beguiling all my sad sad soul into smiling,\n",
            "By the grave and stern decorum of the countenance it wore,\n",
            "\"Though thy crest be shorn and shaven, thou,\" I said, \"art sure no craven,\n",
            "                                     Quoth the Raven, \"Nevermore.\"\n",
            "\n",
            "\"Prophet!\" said I, \"thing of evil!--prophet still, if bird or devil!--\n",
            "                                     Perched, and sat, and nothing more.\"\n",
            "\n",
            "Then, methought, the bird said, \"nevermore.\"\n",
            "\n",
            "Startled at the stillness broken by reply so aptly spoken,\n",
            "\"Doubtless,\" said I, \"what it utters is its only stock and store,\n",
            "\n",
            "\n",
            "[301 | 774.39] loss=0.03 avg=0.03\n",
            "[302 | 776.84] loss=0.03 avg=0.03\n",
            "[303 | 779.29] loss=0.03 avg=0.03\n",
            "[304 | 781.72] loss=0.02 avg=0.03\n",
            "[305 | 784.17] loss=0.04 avg=0.03\n",
            "[306 | 786.63] loss=0.04 avg=0.03\n",
            "[307 | 789.07] loss=0.03 avg=0.03\n",
            "[308 | 791.53] loss=0.03 avg=0.03\n",
            "[309 | 793.97] loss=0.04 avg=0.03\n",
            "[310 | 796.41] loss=0.03 avg=0.03\n",
            "[311 | 798.85] loss=0.03 avg=0.03\n",
            "[312 | 801.29] loss=0.03 avg=0.03\n",
            "[313 | 803.72] loss=0.03 avg=0.03\n",
            "[314 | 806.17] loss=0.03 avg=0.03\n",
            "[315 | 808.60] loss=0.03 avg=0.03\n",
            "[316 | 811.04] loss=0.02 avg=0.03\n",
            "[317 | 813.48] loss=0.02 avg=0.03\n",
            "[318 | 815.92] loss=0.02 avg=0.03\n",
            "[319 | 818.36] loss=0.02 avg=0.03\n",
            "[320 | 820.79] loss=0.03 avg=0.03\n",
            "[321 | 823.23] loss=0.04 avg=0.03\n",
            "[322 | 825.68] loss=0.03 avg=0.03\n",
            "[323 | 828.12] loss=0.03 avg=0.03\n",
            "[324 | 830.57] loss=0.02 avg=0.03\n",
            "[325 | 833.00] loss=0.04 avg=0.03\n",
            "[326 | 835.45] loss=0.04 avg=0.03\n",
            "[327 | 837.90] loss=0.03 avg=0.03\n",
            "[328 | 840.35] loss=0.03 avg=0.03\n",
            "[329 | 842.80] loss=0.02 avg=0.03\n",
            "[330 | 845.26] loss=0.02 avg=0.03\n",
            "[331 | 847.72] loss=0.03 avg=0.03\n",
            "[332 | 850.17] loss=0.03 avg=0.03\n",
            "[333 | 852.63] loss=0.03 avg=0.03\n",
            "[334 | 855.09] loss=0.04 avg=0.03\n",
            "[335 | 857.55] loss=0.02 avg=0.03\n",
            "[336 | 860.00] loss=0.02 avg=0.03\n",
            "[337 | 862.46] loss=0.04 avg=0.03\n",
            "[338 | 864.92] loss=0.02 avg=0.03\n",
            "[339 | 867.37] loss=0.03 avg=0.03\n",
            "[340 | 869.83] loss=0.03 avg=0.03\n",
            "[341 | 872.29] loss=0.03 avg=0.03\n",
            "[342 | 874.75] loss=0.04 avg=0.03\n",
            "[343 | 877.21] loss=0.02 avg=0.03\n",
            "[344 | 879.67] loss=0.03 avg=0.03\n",
            "[345 | 882.13] loss=0.03 avg=0.03\n",
            "[346 | 884.59] loss=0.03 avg=0.03\n",
            "[347 | 887.05] loss=0.03 avg=0.03\n",
            "[348 | 889.51] loss=0.03 avg=0.03\n",
            "[349 | 891.97] loss=0.02 avg=0.03\n",
            "[350 | 894.42] loss=0.02 avg=0.03\n",
            "[351 | 896.88] loss=0.02 avg=0.03\n",
            "[352 | 899.34] loss=0.02 avg=0.03\n",
            "[353 | 901.80] loss=0.03 avg=0.03\n",
            "[354 | 904.26] loss=0.03 avg=0.03\n",
            "[355 | 906.71] loss=0.03 avg=0.03\n",
            "[356 | 909.17] loss=0.04 avg=0.03\n",
            "[357 | 911.62] loss=0.03 avg=0.03\n",
            "[358 | 914.08] loss=0.02 avg=0.03\n",
            "[359 | 916.54] loss=0.03 avg=0.03\n",
            "[360 | 919.00] loss=0.02 avg=0.03\n",
            "[361 | 921.45] loss=0.02 avg=0.03\n",
            "[362 | 923.91] loss=0.02 avg=0.03\n",
            "[363 | 926.37] loss=0.03 avg=0.03\n",
            "[364 | 928.82] loss=0.03 avg=0.03\n",
            "[365 | 931.28] loss=0.02 avg=0.03\n",
            "[366 | 933.74] loss=0.02 avg=0.03\n",
            "[367 | 936.19] loss=0.02 avg=0.03\n",
            "[368 | 938.65] loss=0.03 avg=0.03\n",
            "[369 | 941.11] loss=0.03 avg=0.03\n",
            "[370 | 943.57] loss=0.02 avg=0.03\n",
            "[371 | 946.03] loss=0.03 avg=0.03\n",
            "[372 | 948.50] loss=0.03 avg=0.03\n",
            "[373 | 950.96] loss=0.02 avg=0.03\n",
            "[374 | 953.42] loss=0.02 avg=0.03\n",
            "[375 | 955.88] loss=0.03 avg=0.03\n",
            "[376 | 958.36] loss=0.03 avg=0.03\n",
            "[377 | 960.82] loss=0.03 avg=0.03\n",
            "[378 | 963.27] loss=0.04 avg=0.03\n",
            "[379 | 965.73] loss=0.02 avg=0.03\n",
            "[380 | 968.19] loss=0.02 avg=0.03\n",
            "[381 | 970.64] loss=0.04 avg=0.03\n",
            "[382 | 973.10] loss=0.02 avg=0.03\n",
            "[383 | 975.58] loss=0.02 avg=0.03\n",
            "[384 | 978.04] loss=0.02 avg=0.03\n",
            "[385 | 980.50] loss=0.02 avg=0.03\n",
            "[386 | 982.95] loss=0.02 avg=0.03\n",
            "[387 | 985.41] loss=0.03 avg=0.03\n",
            "[388 | 987.87] loss=0.03 avg=0.03\n",
            "[389 | 990.33] loss=0.03 avg=0.03\n",
            "[390 | 992.78] loss=0.02 avg=0.03\n",
            "[391 | 995.24] loss=0.03 avg=0.03\n",
            "[392 | 997.70] loss=0.03 avg=0.03\n",
            "[393 | 1000.15] loss=0.03 avg=0.03\n",
            "[394 | 1002.61] loss=0.02 avg=0.03\n",
            "[395 | 1005.07] loss=0.02 avg=0.03\n",
            "[396 | 1007.53] loss=0.03 avg=0.03\n",
            "[397 | 1009.99] loss=0.03 avg=0.03\n",
            "[398 | 1012.43] loss=0.06 avg=0.03\n",
            "[399 | 1014.89] loss=0.02 avg=0.03\n",
            "[400 | 1017.35] loss=0.04 avg=0.03\n",
            "======== SAMPLE 1 ========\n",
            "       --\"A stately Raven of the saintly days of yore.\n",
            "Not the least obeisance made he; not a minute stopped or stayed he.\"\n",
            "                                                                  _R. Staudenbaur._\n",
            "\n",
            "\"Perched upon a bust of Pallas just above my chamber door--\n",
            "                                                         Perched, and sat, and nothing more.\"\n",
            "                                                                                                                                                               _R.G. Tietze._\n",
            "\n",
            "\"Wandering from the Nightly shore.\" _Frederick Juengling._\n",
            "\n",
            "\"Till I scarcely more than muttered, 'Other friends have flown before--\n",
            "On the morrow _he_ will leave me, as my hopes have flown before.'\"\n",
            "                                                                               _Frank French._\n",
            "\n",
            "\"Then, upon the velvet sinking, I betook myself to linking\n",
            "Fancy unto fancy.\"                                                       _R. Schelling._\n",
            "\n",
            "\"But whose velvet violet lining with the lamplight gloating o'er\n",
            "                                                                                _George Kruell._\n",
            "\n",
            "\"'Wretch,' I cried, 'thy God hath lent thee--by these angels he hath sent thee\n",
            "Respite--respite and nepenthe from thy memories of Lenore!'\"\n",
            "                                                                                                   _Victor Bernstrom._\n",
            "\n",
            "\"On this home by Horror haunted.\"                          _R. Staudenbaur._\n",
            "\n",
            "\n",
            "                                            \"'Tell me truly, I implore--\n",
            "Is there--_is_ there balm in Gilead?--tell me--tell me, I implore!'\"\n",
            "                                                                \n",
            "\n",
            "[401 | 1029.11] loss=0.04 avg=0.03\n",
            "[402 | 1031.56] loss=0.04 avg=0.03\n",
            "[403 | 1034.01] loss=0.04 avg=0.03\n",
            "[404 | 1036.46] loss=0.03 avg=0.03\n",
            "[405 | 1038.92] loss=0.03 avg=0.03\n",
            "[406 | 1041.37] loss=0.04 avg=0.03\n",
            "[407 | 1043.82] loss=0.03 avg=0.03\n",
            "[408 | 1046.28] loss=0.02 avg=0.03\n",
            "[409 | 1048.74] loss=0.03 avg=0.03\n",
            "[410 | 1051.19] loss=0.04 avg=0.03\n",
            "[411 | 1053.64] loss=0.04 avg=0.03\n",
            "[412 | 1056.08] loss=0.05 avg=0.03\n",
            "[413 | 1058.53] loss=0.03 avg=0.03\n",
            "[414 | 1060.98] loss=0.03 avg=0.03\n",
            "[415 | 1063.43] loss=0.04 avg=0.03\n",
            "[416 | 1065.89] loss=0.04 avg=0.03\n",
            "[417 | 1068.34] loss=0.03 avg=0.03\n",
            "[418 | 1070.80] loss=0.02 avg=0.03\n",
            "[419 | 1073.26] loss=0.03 avg=0.03\n",
            "[420 | 1075.70] loss=0.03 avg=0.03\n",
            "[421 | 1078.16] loss=0.02 avg=0.03\n",
            "[422 | 1080.60] loss=0.03 avg=0.03\n",
            "[423 | 1083.05] loss=0.02 avg=0.03\n",
            "[424 | 1085.50] loss=0.02 avg=0.03\n",
            "[425 | 1087.96] loss=0.02 avg=0.03\n",
            "[426 | 1090.42] loss=0.03 avg=0.03\n",
            "[427 | 1092.87] loss=0.03 avg=0.03\n",
            "[428 | 1095.33] loss=0.03 avg=0.03\n",
            "[429 | 1097.79] loss=0.02 avg=0.03\n",
            "[430 | 1100.24] loss=0.02 avg=0.03\n",
            "[431 | 1102.70] loss=0.02 avg=0.03\n",
            "[432 | 1105.15] loss=0.03 avg=0.03\n",
            "[433 | 1107.61] loss=0.03 avg=0.03\n",
            "[434 | 1110.05] loss=0.02 avg=0.03\n",
            "[435 | 1112.51] loss=0.03 avg=0.03\n",
            "[436 | 1114.96] loss=0.03 avg=0.03\n",
            "[437 | 1117.42] loss=0.02 avg=0.03\n",
            "[438 | 1119.87] loss=0.03 avg=0.03\n",
            "[439 | 1122.33] loss=0.02 avg=0.03\n",
            "[440 | 1124.79] loss=0.03 avg=0.03\n",
            "[441 | 1127.25] loss=0.03 avg=0.03\n",
            "[442 | 1129.70] loss=0.03 avg=0.03\n",
            "[443 | 1132.16] loss=0.03 avg=0.03\n",
            "[444 | 1134.62] loss=0.03 avg=0.03\n",
            "[445 | 1137.07] loss=0.02 avg=0.03\n",
            "[446 | 1139.53] loss=0.03 avg=0.03\n",
            "[447 | 1141.99] loss=0.02 avg=0.03\n",
            "[448 | 1144.45] loss=0.03 avg=0.03\n",
            "[449 | 1146.91] loss=0.03 avg=0.03\n",
            "[450 | 1149.37] loss=0.03 avg=0.03\n",
            "[451 | 1151.83] loss=0.02 avg=0.03\n",
            "[452 | 1154.29] loss=0.03 avg=0.03\n",
            "[453 | 1156.75] loss=0.03 avg=0.03\n",
            "[454 | 1159.21] loss=0.02 avg=0.03\n",
            "[455 | 1161.66] loss=0.03 avg=0.03\n",
            "[456 | 1164.12] loss=0.03 avg=0.03\n",
            "[457 | 1166.58] loss=0.03 avg=0.03\n",
            "[458 | 1169.04] loss=0.03 avg=0.03\n",
            "[459 | 1171.50] loss=0.02 avg=0.03\n",
            "[460 | 1173.97] loss=0.02 avg=0.03\n",
            "[461 | 1176.43] loss=0.02 avg=0.03\n",
            "[462 | 1178.89] loss=0.02 avg=0.03\n",
            "[463 | 1181.35] loss=0.02 avg=0.03\n",
            "[464 | 1183.80] loss=0.02 avg=0.03\n",
            "[465 | 1186.28] loss=0.03 avg=0.03\n",
            "[466 | 1188.74] loss=0.02 avg=0.03\n",
            "[467 | 1191.19] loss=0.03 avg=0.03\n",
            "[468 | 1193.65] loss=0.04 avg=0.03\n",
            "[469 | 1196.11] loss=0.03 avg=0.03\n",
            "[470 | 1198.56] loss=0.02 avg=0.03\n",
            "[471 | 1201.02] loss=0.02 avg=0.03\n",
            "[472 | 1203.48] loss=0.02 avg=0.03\n",
            "[473 | 1205.93] loss=0.01 avg=0.03\n",
            "[474 | 1208.39] loss=0.02 avg=0.03\n",
            "[475 | 1210.85] loss=0.03 avg=0.03\n",
            "[476 | 1213.31] loss=0.02 avg=0.03\n",
            "[477 | 1215.77] loss=0.02 avg=0.03\n",
            "[478 | 1218.22] loss=0.03 avg=0.03\n",
            "[479 | 1220.70] loss=0.03 avg=0.03\n",
            "[480 | 1223.15] loss=0.02 avg=0.03\n",
            "[481 | 1225.61] loss=0.02 avg=0.03\n",
            "[482 | 1228.08] loss=0.03 avg=0.03\n",
            "[483 | 1230.53] loss=0.03 avg=0.03\n",
            "[484 | 1232.99] loss=0.03 avg=0.03\n",
            "[485 | 1235.44] loss=0.02 avg=0.03\n",
            "[486 | 1237.90] loss=0.03 avg=0.03\n",
            "[487 | 1240.36] loss=0.02 avg=0.03\n",
            "[488 | 1242.81] loss=0.03 avg=0.03\n",
            "[489 | 1245.27] loss=0.02 avg=0.03\n",
            "[490 | 1247.73] loss=0.02 avg=0.03\n",
            "[491 | 1250.18] loss=0.02 avg=0.03\n",
            "[492 | 1252.64] loss=0.03 avg=0.03\n",
            "[493 | 1255.09] loss=0.03 avg=0.03\n",
            "[494 | 1257.55] loss=0.02 avg=0.03\n",
            "[495 | 1260.01] loss=0.03 avg=0.03\n",
            "[496 | 1262.46] loss=0.02 avg=0.03\n",
            "[497 | 1264.90] loss=0.03 avg=0.03\n",
            "[498 | 1267.35] loss=0.02 avg=0.03\n",
            "[499 | 1269.82] loss=0.02 avg=0.03\n",
            "[500 | 1272.27] loss=0.02 avg=0.03\n",
            "Saving checkpoint/run1/model-500\n",
            "Frederick Juengling._\n",
            "\n",
            "\"Then, upon the velvet sinking, I betook myself to linking\n",
            "Fancy unto fancy.\"                                 _R. Schelling._\n",
            "\n",
            "\"But whose velvet violet lining with the lamplight gloating o'er\n",
            "                                       _She_ shall press, ah, nevermore!\"\n",
            "                                                   _George Kruell._\n",
            "\n",
            "\"'Wretch,' I cried, 'thy God hath lent thee--by these angels he hath sent thee\n",
            "Respite--respite and nepenthe from thy memories of Lenore!'\"\n",
            "                                               _Victor Bernstrom._\n",
            "\n",
            "\"On this home by Horror haunted.\"                  _R. Staudenbaur._\n",
            "\n",
            "                             \"'Tell me truly, I implore--\n",
            "Is there--_is_ there balm in Gilead?--tell me--tell me, I implore!'\"\n",
            "                                                    _W. Zimmermann._\n",
            "\n",
            "\"'Tell this soul with sorrow laden if, within the distant Aidenn,\n",
            "It shall clasp a sainted maiden whom the angels name Lenore.'\"\n",
            "                                                    _F.S. King._\n",
            "\n",
            "\"'Be that word our sign of parting, bird or fiend!' I shrieked, upstarting.\"\n",
            "                                                  _W. Zimmermann._\n",
            "\n",
            "\"'Get thee back into the tempest and the Night's Plutonian shore!'\"\n",
            "                                                   _Robert Hoskin._\n",
            "\n",
            "\"And my soul from out that shadow that lies floating on the floor\n",
            "                                         Shall be lifted--nevermore!\"\n",
            "                                                  _R.G. Tietze._\n",
            "\n",
            "The secret of the Sphinx.                             _R. Staudenbaur._\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "COMMENT ON THE POEM.\n",
            "\n",
            "\n",
            "The secret of a poem, no less than a jest's prosperity, lies in the ear of\n",
            "him that hears it. Yield to its spell, accept the poet's mood: this, after\n",
            "all, is what the sages answer when you ask them of its value. Even though\n",
            "the poet himself, in his other mood, tell you that his art is but sleight\n",
            "of hand, his food enchanter's food, and offer to show you the trick of\n",
            "it,--believe him not. Wait for his prophetic hour; then give yourself to\n",
            "his passion, his joy or pain. \"We are in Love's hand to-day!\" sings\n",
            "Gautier, in Swinburne's buoyant\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}